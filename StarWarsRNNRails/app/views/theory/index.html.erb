<div class="body-wrapper container-fluid">
	<div class="info-body container">
		</br>
		<h3 style="text-align:center;">What It Does</h3>
		<p>The specific language model we’re using is an LSTM, a specific type of recurrent neural network. The LSTM reads the scripts for Star Wars films one character at a time, and tries to predict the next character. After a few hours, it becomes very good at doing this. We can then give it a starting seed, and then have it predict the next page of text. By doing this, it is effectively generating new content inspired by the patterns it finds in the training data.</p>

		<h3 style="text-align:center;">How We Built It</h3>
		<p>We used Google’s TensorFlow API to build our LSTM. TensorFlow allows us to have a strong amount of control over our network’s architecture, and it has good support for GPU computation. This meant that we were able to train efficiently on an AWS GPU instance. The network itself uses one-hot encodings for every character in the training text, and then feeds this into two stacked hidden layers per LSTM cell, and then uses a softmax classifier to predict the one-hot vector of the next character. Using the Adam optimization algorithm, we can minimize the resulting perplexity with respect to the network weights.</p>

		<p>After training, we can generate text by feeding in a short seed word, and then incrementally computing the hidden states and then the predictions for the next character. We adjust the predictions using a temperature which can control how original vs. faithful the new text is compared to the training data, and then sample the character from the adjusted distribution.</p>

	</div>

</div>